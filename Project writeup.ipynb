{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1125e9fd",
   "metadata": {},
   "source": [
    "#Classifying Defensive Coverages with Convolutional Nueral Network\n",
    "##By Spencer Kerch\n",
    "###Credit and masssive thanks to Ben Baldwin for his Torch for R tutorial and his ngscleanr documentation for helping me through this "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0440ae",
   "metadata": {},
   "source": [
    "\n",
    "For my Deep Learning and AI Ethics class taught by Dr. Scott Hawley, our final project was to build some sort of deep learning algorothim we had covered during the course and present it to the class. As a reader of Open Source Football I wanted to recreate the defensive coverage classifier from his [Computer Vision with NFL Player Tracking Data using torch for R](https://www.opensourcefootball.com/posts/2021-05-31-computer-vision-in-r-using-torch/) post. When I first read his blog post nearly all of it went over my head. However, after learning more about how nueral networks work from my class the proccess made much more sense. The classifier I made is more simple to reflect my skill and understanding of neural networks. Im am very proud of how the model turned out as this was easily one of the most difficult projects I've done in the time I have been doing statistical programming - a little over a year. If while reading this you have any feedback for me on my code or my process please contact me I'd love to hear it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f72550",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54e3e1c9",
   "metadata": {},
   "source": [
    "I first read in data I got from the NFL's [2021 Big Data Bowl](https://www.kaggle.com/c/nfl-big-data-bowl-2021/data) as well as established a classes list with only the names of the different coverages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27291deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "coverages = pd.read_csv(\"/Users/spencerkerch/Desktop/proj/coverages_week1.csv\")  #contains the coverage for each play in week 1(only week 1 data was provided for coverages)\n",
    "plays = pd.read_csv(\"/Users/spencerkerch/Desktop/proj/plays.csv\")\n",
    "week1 = pd.read_csv(\"/Users/spencerkerch/Desktop/proj/week1.csv\") #week 1 tracking data\n",
    "games =  pd.read_csv(\"/Users/spencerkerch/Desktop/proj/games.csv\")\n",
    "classes = [\"Cover 0 Man\",\"Cover 1 Man\",\"Cover 2 Man\",\"Cover 2 Zone\", \"Cover 3 Zone\", \"Cover 4 Zone\",\"Cover 6 Zone\", \"Prevent Zone\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57118537",
   "metadata": {},
   "source": [
    "Now I filter the dataframes I created above to get the data I need to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2042d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(week1,coverages, on = [\"gameId\",\"playId\"] )\n",
    "import numpy as np\n",
    "#I dont need Football or QB data\n",
    "merged_df = merged_df.query('displayName != \"Football\"')\n",
    "merged_df = merged_df.query('position != \"QB\"')\n",
    "#I will only use the tracking data for the ball snap frame to predict coverages\n",
    "merged_df = merged_df.query('event == \"ball_snap\"')\n",
    "#I need to distinguish defense from offense\n",
    "merged_df.position.unique()\n",
    "offense = [\"RB\",\"WR\",\"FB\",\"HB\",\"TE\"]\n",
    "merged_df = merged_df.assign(defense = [0 if position in offense else 1 for position in merged_df['position']])\n",
    "merged_df.head\n",
    "merged_df = pd.merge(merged_df,plays,on=[\"gameId\",\"playId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d32259",
   "metadata": {},
   "source": [
    "I understand that since coverages are often disguised pre-snap, using a singular frame at the snap of the ball will likely cause misclassifications. But for simplicity's sake I'm chosing to do this. * I'm also curious to see if the misclassifications can be used to help determine what looks teams like to fake *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b2df9",
   "metadata": {},
   "source": [
    "Now I need to clean the data to get accurate values and create other values I will use in the model. In Ben's blog post he simply used his ngscleanr package to get most of this done; however, there appears to be no equivelant python package. So using math and the [ngscleanr documentation](https://github.com/guga31bb/ngscleanR/blob/master/R/cleaning_functions.R) I do the required calcualtions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0270e6",
   "metadata": {},
   "source": [
    "In Bens model he includes orientation or more specifically orientation to the qb. I'll leave these out to make the calculations easier on me and save time. If I included these it would have no doubt benefited the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "169b013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing x and y coords to be the same no matter the play direction\n",
    "merged_df['x'] = np.where(merged_df['playDirection'] == 'left', 120 - merged_df['x'] , merged_df['x'])\n",
    "merged_df['y'] = np.where(merged_df['playDirection'] == 'left', (160/3) - merged_df['y'] , merged_df['y'])\n",
    "#approximation of the line of scrimmage\n",
    "merged_df['los_x'] = np.where(merged_df['playDirection'] == 'left', 120 - merged_df['absoluteYardlineNumber'] , merged_df['absoluteYardlineNumber'])\n",
    "#distance from line of scrimage\n",
    "merged_df['dist_from_los'] = merged_df['x'] - merged_df['los_x']\n",
    "#changing player direction to be the same no matter the play direction, as well converting to radians\n",
    "merged_df['dir'] = np.where(merged_df['playDirection'] == 'left', merged_df['dir'] + 180 , merged_df['dir'])\n",
    "merged_df['dir'] = np.where(merged_df['dir'] > 360,merged_df['dir'] - 360, merged_df['dir'])\n",
    "merged_df['dir_rad'] = np.pi *(merged_df['dir']/180)\n",
    "#direction in the x and y\n",
    "merged_df['dir_x'] = np.sin(merged_df['dir_rad'])\n",
    "merged_df['dir_y'] = np.cos(merged_df['dir_rad'])\n",
    "#speed in the x and y\n",
    "merged_df['s_x'] = merged_df['dir_x'] * merged_df['s']\n",
    "merged_df['s_y'] = merged_df['dir_y'] * merged_df['s']\n",
    "#acceleration in the x and y\n",
    "merged_df['a_x'] = merged_df['dir_x'] * merged_df['a']\n",
    "merged_df['a_y'] = merged_df['dir_y'] * merged_df['a']\n",
    "#merged_df['play'] = str( merged_df['gameId']) + str(merged_df['playId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46b22de",
   "metadata": {},
   "source": [
    "The next step is to seperate the offense and defese tracking data into 2 data frames. I simply filter around the defense variable I created earlier. I also rename any offense variable to o_variable to differentiate offense data from defense data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2147993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of            gameId  playId    o_x        o_y     o_s_x     o_s_y     o_a_x  \\\n",
      "1      2018090600      75  28.64   9.193333 -0.000000 -0.000000 -0.007645   \n",
      "3      2018090600      75  29.22  17.173333  0.000000 -0.000000  0.000000   \n",
      "5      2018090600      75  21.75  26.713333  0.000000  0.000000  0.000000   \n",
      "9      2018090600      75  28.70  31.483333 -0.000000  0.000000 -0.005749   \n",
      "11     2018090600      75  24.88  26.633333  0.000000  0.000000  0.000000   \n",
      "...           ...     ...    ...        ...       ...       ...       ...   \n",
      "13074  2018091001    3976  54.56   9.923333  0.314237 -0.060456  0.883793   \n",
      "13076  2018091001    3976  55.29  36.183333  0.019148 -0.035119  0.019148   \n",
      "13080  2018091001    3976  56.09  23.173333  0.089838  0.005400  0.099820   \n",
      "13081  2018091001    3976  54.50  16.763333  0.400951 -0.085664  0.156469   \n",
      "13086  2018091001    3976  51.69  32.233333  0.042254  0.055809  0.042254   \n",
      "\n",
      "          o_a_y  \n",
      "1     -0.006447  \n",
      "3     -0.000000  \n",
      "5      0.000000  \n",
      "9      0.008183  \n",
      "11     0.000000  \n",
      "...         ...  \n",
      "13074 -0.170032  \n",
      "13076 -0.035119  \n",
      "13080  0.006000  \n",
      "13081 -0.033430  \n",
      "13086  0.055809  \n",
      "\n",
      "[5132 rows x 8 columns]>\n",
      "           gameId  playId      nflId      x          y       s_x       s_y  \\\n",
      "0      2018090600      75    79848.0  31.11  16.833333 -0.005941 -0.008044   \n",
      "2      2018090600      75  2495613.0  33.53  31.303333 -0.178377 -0.024118   \n",
      "4      2018090600      75  2534832.0  46.39  24.633333  0.019592  0.004022   \n",
      "6      2018090600      75  2552315.0  33.37  22.213333 -0.064783 -0.026517   \n",
      "7      2018090600      75  2552689.0  37.37  32.833333  0.015487  0.025693   \n",
      "...           ...     ...        ...    ...        ...       ...       ...   \n",
      "13082  2018091001    3976  2552488.0  62.39  38.783333  1.485516  0.366253   \n",
      "13083  2018091001    3976  2553722.0  57.04  24.953333 -0.138887  0.017619   \n",
      "13084  2018091001    3976  2556277.0  60.81  24.913333 -0.070698 -0.070723   \n",
      "13085  2018091001    3976  2556593.0  59.15  33.283333 -0.223489  0.458424   \n",
      "13087  2018091001    3976  2558183.0  66.55  32.223333  1.396686  0.193309   \n",
      "\n",
      "            a_x       a_y  dist_from_los  \n",
      "0     -0.005941 -0.008044           1.11  \n",
      "2     -0.069369 -0.009379           3.53  \n",
      "4      0.019592  0.004022          16.39  \n",
      "6     -0.064783 -0.026517           3.37  \n",
      "7      0.015487  0.025693           7.37  \n",
      "...         ...       ...            ...  \n",
      "13082  0.466044  0.114903           5.39  \n",
      "13083 -0.138887  0.017619           0.04  \n",
      "13084 -0.190886 -0.190952           3.81  \n",
      "13085 -0.403157  0.826961           2.15  \n",
      "13087  0.693390  0.095969           9.55  \n",
      "\n",
      "[7956 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "offense_df = merged_df.query('defense == 0 & position != \"QB\"')\n",
    "#print(offense_df['position'])\n",
    "offense_df = offense_df.rename(columns={'x' : 'o_x', 'y' : 'o_y', 's_x':'o_s_x', 's_y':'o_s_y', 'a_x':'o_a_x', 'a_y':'o_a_y'})\n",
    "#print(offense_df.head)\n",
    "offense_df = offense_df[['gameId', 'playId', 'o_x', 'o_y', 'o_s_x', 'o_s_y','o_a_x','o_a_y']]\n",
    "#print(offense_df.head)\n",
    "#offense_df = offense_df.drop(offense_df.columns[[3,5]],axis=1)\n",
    "print(offense_df.head)\n",
    "defense_df = merged_df.query('defense == 1')\n",
    "defense_df = defense_df[['gameId', 'playId','nflId', 'x', 'y', 's_x', 's_y','a_x','a_y', 'dist_from_los']]\n",
    "print(defense_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1c281",
   "metadata": {},
   "source": [
    "Next I merge the offense and defense df into one df for each play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bd14cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of            gameId  playId      nflId      x          y       s_x       s_y  \\\n",
      "0      2018090600      75    79848.0  31.11  16.833333 -0.005941 -0.008044   \n",
      "1      2018090600      75    79848.0  31.11  16.833333 -0.005941 -0.008044   \n",
      "2      2018090600      75    79848.0  31.11  16.833333 -0.005941 -0.008044   \n",
      "3      2018090600      75    79848.0  31.11  16.833333 -0.005941 -0.008044   \n",
      "4      2018090600      75    79848.0  31.11  16.833333 -0.005941 -0.008044   \n",
      "...           ...     ...        ...    ...        ...       ...       ...   \n",
      "39718  2018091001    3976  2558183.0  66.55  32.223333  1.396686  0.193309   \n",
      "39719  2018091001    3976  2558183.0  66.55  32.223333  1.396686  0.193309   \n",
      "39720  2018091001    3976  2558183.0  66.55  32.223333  1.396686  0.193309   \n",
      "39721  2018091001    3976  2558183.0  66.55  32.223333  1.396686  0.193309   \n",
      "39722  2018091001    3976  2558183.0  66.55  32.223333  1.396686  0.193309   \n",
      "\n",
      "            a_x       a_y  dist_from_los    o_x        o_y     o_s_x  \\\n",
      "0     -0.005941 -0.008044           1.11  28.64   9.193333 -0.000000   \n",
      "1     -0.005941 -0.008044           1.11  29.22  17.173333  0.000000   \n",
      "2     -0.005941 -0.008044           1.11  21.75  26.713333  0.000000   \n",
      "3     -0.005941 -0.008044           1.11  28.70  31.483333 -0.000000   \n",
      "4     -0.005941 -0.008044           1.11  24.88  26.633333  0.000000   \n",
      "...         ...       ...            ...    ...        ...       ...   \n",
      "39718  0.693390  0.095969           9.55  54.56   9.923333  0.314237   \n",
      "39719  0.693390  0.095969           9.55  55.29  36.183333  0.019148   \n",
      "39720  0.693390  0.095969           9.55  56.09  23.173333  0.089838   \n",
      "39721  0.693390  0.095969           9.55  54.50  16.763333  0.400951   \n",
      "39722  0.693390  0.095969           9.55  51.69  32.233333  0.042254   \n",
      "\n",
      "          o_s_y     o_a_x     o_a_y  \n",
      "0     -0.000000 -0.007645 -0.006447  \n",
      "1     -0.000000  0.000000 -0.000000  \n",
      "2      0.000000  0.000000  0.000000  \n",
      "3      0.000000 -0.005749  0.008183  \n",
      "4      0.000000  0.000000  0.000000  \n",
      "...         ...       ...       ...  \n",
      "39718 -0.060456  0.883793 -0.170032  \n",
      "39719 -0.035119  0.019148 -0.035119  \n",
      "39720  0.005400  0.099820  0.006000  \n",
      "39721 -0.085664  0.156469 -0.033430  \n",
      "39722  0.055809  0.042254  0.055809  \n",
      "\n",
      "[39723 rows x 16 columns]>\n"
     ]
    }
   ],
   "source": [
    "df = defense_df.merge(offense_df, how = 'left', on = ['gameId','playId'])\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90edfbd8",
   "metadata": {},
   "source": [
    "Next, to get variables that are useful for the model I create \"diff\" variables for the difference in the offense and defense. This shows each defensive player in relation to each offensive player. I'll leave the qb out of the group of offensive players to only show the defender in relation to possible recievers, which is needed to determine coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d1dfef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of            nflId  dist_from_los          y  diff_x  diff_y\n",
       "0        79848.0           1.11  16.833333   -2.47   -7.64\n",
       "1        79848.0           1.11  16.833333   -1.89    0.34\n",
       "2        79848.0           1.11  16.833333   -9.36    9.88\n",
       "3        79848.0           1.11  16.833333   -2.41   14.65\n",
       "4        79848.0           1.11  16.833333   -6.23    9.80\n",
       "...          ...            ...        ...     ...     ...\n",
       "39718  2558183.0           9.55  32.223333  -11.99  -22.30\n",
       "39719  2558183.0           9.55  32.223333  -11.26    3.96\n",
       "39720  2558183.0           9.55  32.223333  -10.46   -9.05\n",
       "39721  2558183.0           9.55  32.223333  -12.05  -15.46\n",
       "39722  2558183.0           9.55  32.223333  -14.86    0.01\n",
       "\n",
       "[39723 rows x 5 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['diff_x'] = df['o_x'] - df['x']\n",
    "df['diff_y'] = df['o_y'] - df['y']\n",
    "df['diff_s_x'] = df['o_s_x'] - df['s_x']\n",
    "df['diff_s_y'] = df['o_s_y'] - df['s_y']\n",
    "df['diff_a_x'] = df['o_a_x'] - df['a_x']\n",
    "df['diff_a_y'] = df['o_a_y'] - df['a_y']\n",
    "df = df[['gameId','playId','nflId','dist_from_los', 'y', 's_x', 's_y','a_x','a_y','diff_x','diff_y','diff_s_x','diff_s_y','diff_a_x','diff_a_y']]\n",
    "df[['nflId','dist_from_los','y','diff_x',\"diff_y\"]].head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85606273",
   "metadata": {},
   "source": [
    "Now I need to create an index holding each play and it's respective coverage. Combining the gameId and playId for each play creates a unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a8ccb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 play      coverage\n",
      "0       2018090600_75  Cover 3 Zone\n",
      "1      2018090600_146  Cover 3 Zone\n",
      "2      2018090600_168  Cover 3 Zone\n",
      "3      2018090600_190  Cover 3 Zone\n",
      "4      2018090600_256   Cover 0 Man\n",
      "...               ...           ...\n",
      "1023   2018091000_372   Cover 1 Man\n",
      "1024  2018091000_2303   Cover 1 Man\n",
      "1025  2018091000_2904  Cover 2 Zone\n",
      "1026   2018091001_741  Cover 2 Zone\n",
      "1027  2018091001_1181  Cover 4 Zone\n",
      "\n",
      "[1028 rows x 2 columns]\n",
      "1027\n",
      "           gameId  playId      nflId  dist_from_los          y       s_x  \\\n",
      "0      2018090600      75    79848.0           1.11  16.833333 -0.005941   \n",
      "1      2018090600      75    79848.0           1.11  16.833333 -0.005941   \n",
      "2      2018090600      75    79848.0           1.11  16.833333 -0.005941   \n",
      "3      2018090600      75    79848.0           1.11  16.833333 -0.005941   \n",
      "4      2018090600      75    79848.0           1.11  16.833333 -0.005941   \n",
      "...           ...     ...        ...            ...        ...       ...   \n",
      "39718  2018091001    3976  2558183.0           9.55  32.223333  1.396686   \n",
      "39719  2018091001    3976  2558183.0           9.55  32.223333  1.396686   \n",
      "39720  2018091001    3976  2558183.0           9.55  32.223333  1.396686   \n",
      "39721  2018091001    3976  2558183.0           9.55  32.223333  1.396686   \n",
      "39722  2018091001    3976  2558183.0           9.55  32.223333  1.396686   \n",
      "\n",
      "            s_y       a_x       a_y  diff_x  diff_y  diff_s_x  diff_s_y  \\\n",
      "0     -0.008044 -0.005941 -0.008044   -2.47   -7.64  0.005941  0.008044   \n",
      "1     -0.008044 -0.005941 -0.008044   -1.89    0.34  0.005941  0.008044   \n",
      "2     -0.008044 -0.005941 -0.008044   -9.36    9.88  0.005941  0.008044   \n",
      "3     -0.008044 -0.005941 -0.008044   -2.41   14.65  0.005941  0.008044   \n",
      "4     -0.008044 -0.005941 -0.008044   -6.23    9.80  0.005941  0.008044   \n",
      "...         ...       ...       ...     ...     ...       ...       ...   \n",
      "39718  0.193309  0.693390  0.095969  -11.99  -22.30 -1.082449 -0.253765   \n",
      "39719  0.193309  0.693390  0.095969  -11.26    3.96 -1.377538 -0.228429   \n",
      "39720  0.193309  0.693390  0.095969  -10.46   -9.05 -1.306848 -0.187909   \n",
      "39721  0.193309  0.693390  0.095969  -12.05  -15.46 -0.995735 -0.278973   \n",
      "39722  0.193309  0.693390  0.095969  -14.86    0.01 -1.354432 -0.137501   \n",
      "\n",
      "       diff_a_x  diff_a_y             play  \n",
      "0     -0.001704  0.001597    2018090600_75  \n",
      "1      0.005941  0.008044    2018090600_75  \n",
      "2      0.005941  0.008044    2018090600_75  \n",
      "3      0.000193  0.016226    2018090600_75  \n",
      "4      0.005941  0.008044    2018090600_75  \n",
      "...         ...       ...              ...  \n",
      "39718  0.190402 -0.266001  2018091001_3976  \n",
      "39719 -0.674242 -0.131088  2018091001_3976  \n",
      "39720 -0.593570 -0.089969  2018091001_3976  \n",
      "39721 -0.536921 -0.129399  2018091001_3976  \n",
      "39722 -0.651136 -0.040160  2018091001_3976  \n",
      "\n",
      "[39723 rows x 16 columns]\n",
      "           nflId  dist_from_los          y       s_x       s_y       a_x  \\\n",
      "0        79848.0           1.11  16.833333 -0.005941 -0.008044 -0.005941   \n",
      "1        79848.0           1.11  16.833333 -0.005941 -0.008044 -0.005941   \n",
      "2        79848.0           1.11  16.833333 -0.005941 -0.008044 -0.005941   \n",
      "3        79848.0           1.11  16.833333 -0.005941 -0.008044 -0.005941   \n",
      "4        79848.0           1.11  16.833333 -0.005941 -0.008044 -0.005941   \n",
      "...          ...            ...        ...       ...       ...       ...   \n",
      "39718  2558183.0           9.55  32.223333  1.396686  0.193309  0.693390   \n",
      "39719  2558183.0           9.55  32.223333  1.396686  0.193309  0.693390   \n",
      "39720  2558183.0           9.55  32.223333  1.396686  0.193309  0.693390   \n",
      "39721  2558183.0           9.55  32.223333  1.396686  0.193309  0.693390   \n",
      "39722  2558183.0           9.55  32.223333  1.396686  0.193309  0.693390   \n",
      "\n",
      "            a_y  diff_x  diff_y  diff_s_x  diff_s_y  diff_a_x  diff_a_y  \\\n",
      "0     -0.008044   -2.47   -7.64  0.005941  0.008044 -0.001704  0.001597   \n",
      "1     -0.008044   -1.89    0.34  0.005941  0.008044  0.005941  0.008044   \n",
      "2     -0.008044   -9.36    9.88  0.005941  0.008044  0.005941  0.008044   \n",
      "3     -0.008044   -2.41   14.65  0.005941  0.008044  0.000193  0.016226   \n",
      "4     -0.008044   -6.23    9.80  0.005941  0.008044  0.005941  0.008044   \n",
      "...         ...     ...     ...       ...       ...       ...       ...   \n",
      "39718  0.095969  -11.99  -22.30 -1.082449 -0.253765  0.190402 -0.266001   \n",
      "39719  0.095969  -11.26    3.96 -1.377538 -0.228429 -0.674242 -0.131088   \n",
      "39720  0.095969  -10.46   -9.05 -1.306848 -0.187909 -0.593570 -0.089969   \n",
      "39721  0.095969  -12.05  -15.46 -0.995735 -0.278973 -0.536921 -0.129399   \n",
      "39722  0.095969  -14.86    0.01 -1.354432 -0.137501 -0.651136 -0.040160   \n",
      "\n",
      "                  play  \n",
      "0        2018090600_75  \n",
      "1        2018090600_75  \n",
      "2        2018090600_75  \n",
      "3        2018090600_75  \n",
      "4        2018090600_75  \n",
      "...                ...  \n",
      "39718  2018091001_3976  \n",
      "39719  2018091001_3976  \n",
      "39720  2018091001_3976  \n",
      "39721  2018091001_3976  \n",
      "39722  2018091001_3976  \n",
      "\n",
      "[39723 rows x 14 columns]\n",
      "                 play      coverage     i\n",
      "0       2018090600_75  Cover 3 Zone     1\n",
      "1      2018090600_146  Cover 3 Zone     2\n",
      "2      2018090600_168  Cover 3 Zone     3\n",
      "3      2018090600_190  Cover 3 Zone     4\n",
      "4      2018090600_256   Cover 0 Man     5\n",
      "...               ...           ...   ...\n",
      "1023   2018091000_372   Cover 1 Man  1024\n",
      "1024  2018091000_2303   Cover 1 Man  1025\n",
      "1025  2018091000_2904  Cover 2 Zone  1026\n",
      "1026   2018091001_741  Cover 2 Zone  1027\n",
      "1027  2018091001_1181  Cover 4 Zone  1028\n",
      "\n",
      "[1028 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "play_index = plays.merge(coverages,on = [\"gameId\",\"playId\"])\n",
    "play_index['play'] = play_index['gameId'].map(str) + \"_\" + play_index['playId'].map(str)\n",
    "play_index = play_index[['play', 'coverage']]\n",
    "print(play_index)\n",
    "n_plays = play_index['play'].size -1 #6 offense\n",
    "print(n_plays)\n",
    "df['play'] = df['gameId'].map(str) + \"_\" + df['playId'].map(str)\n",
    "print(df)\n",
    "df = df.drop(columns = ['gameId','playId'])\n",
    "print(df)\n",
    "play_index['i'] = play_index.index + 1\n",
    "print(play_index)\n",
    "#drop 479 due to nulls in that play\n",
    "play_index = play_index.drop(index = 479)#6 offense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672fb1de",
   "metadata": {},
   "source": [
    "Thats it for data prep and cleaning! I'll admit I'm somewhat new to python especially compared to my experience with R and this part of the project did take a decent amount of time. But now I get to move on to more deep learning and nueral network stuff. It's now time to create and fill my tensors. A tensor is a torch object used in nueral networks that is a multidimensional matrix. I want the tensor to be 1027(number of plays)x12(number of features)x11(possible defenders on a play)x5(possible players on offense on a play). It is very hard for me to picture things once their past the 3rd dimension, but I choose to interpert this in my brain as for every play we use the 12 features for each defender in relation to each receiver on offense to predict the coverages. Anyways, I'll need to create an empty matrix of size 1027,12,11,5 so I'll use torch.zeros to fill a matrix that size all with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "517cad08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./opt/anaconda3/lib/python3.9/site-packages (1.10.0)\n",
      "Requirement already satisfied: torchvision in ./opt/anaconda3/lib/python3.9/site-packages (0.11.1)\n",
      "Requirement already satisfied: typing-extensions in ./opt/anaconda3/lib/python3.9/site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: numpy in ./opt/anaconda3/lib/python3.9/site-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in ./opt/anaconda3/lib/python3.9/site-packages (from torchvision) (8.4.0)\n",
      "torch.Size([1027, 12, 11, 5])\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "n_features = 12 #'dist_from_los', 'y', 's_x', 's_y','a_x','a_y','diff_x','diff_y','diff_s_x','diff_s_y','diff_a_x','diff_a_y'\n",
    "data_tensor = torch.zeros((n_plays,n_features,11,5), dtype = torch.float32)\n",
    "print(data_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a3e7b5",
   "metadata": {},
   "source": [
    "This function fills each tensor and does so by transposing the play_df into the right shape. Transposing flips the play so if it is size 35x12, it becomes 12x35. Making the variables the row and each instance of a defender in relation to the offense the collumns. The final line makes sure that when filling the tensor, it cuts off every 5, creating the shape I want of 12 7x5 matrices. Each matrix relating to a variable, each row relating to a defender, and each collumn relating to the defender in relation to the offense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34e0bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_tensor(row):\n",
    "    i = row\n",
    "    row = play_index[row - 1:row]\n",
    "    #i = row['i']\n",
    "    playId = row.iloc[0]['play']\n",
    "    play_df = df.query('play == @playId')\n",
    "    play_df = play_df.drop(columns = 'play')\n",
    "    #number of defenders\n",
    "    defenders = pd.unique(play_df['nflId']).size\n",
    "    #number of offense if there are 35 rows and 7 defenders that means there are 5 players on offense we are tracking. 35/7=5\n",
    "    offense_players = play_df['nflId'].size / defenders\n",
    "    play_df = play_df.drop(columns = 'nflId')\n",
    "    t = play_df.T\n",
    "    #Thanks to Ben and Corey in the nflfastr discord for teaching me about python indexing so I could fill the tensor\n",
    "    data_tensor[i-1,:,:defenders,:int(offense_players)] = torch.from_numpy(t.values).view(-1,defenders,int(offense_players))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f607ce4",
   "metadata": {},
   "source": [
    "The loop below fills our tensor over all the 1027 plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "422ebb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n = 1\n",
    "while n <= n_plays:\n",
    "    fill_tensor(n)\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4db7d",
   "metadata": {},
   "source": [
    "I still dont exactly know why, but some plays returned all nulls. Im sure it has to do with my code. I didnt have enough time to fix the problem as finals was approaching so I'll shorten the data to where I notice a pattern of every play being null began. Their were still num nan's here and there however, so I use nan_to_num to allow my network to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acdd53bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([985, 12, 11, 5])\n",
      "torch.Size([985, 12, 11, 5])\n"
     ]
    }
   ],
   "source": [
    "data_tensor = data_tensor[0:985]\n",
    "print(data_tensor.shape)\n",
    "data_tensor = torch.nan_to_num(data_tensor)\n",
    "print(data_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64714bd",
   "metadata": {},
   "source": [
    "For my net to correctly classify coverages, I need to turn the coverages into a factor and turn it into a 1xn_plays sized tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c211cff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 0, 0, ..., 5, 5, 4]), Index(['Cover 3 Zone', 'Cover 0 Man', 'Cover 1 Man', 'Cover 2 Man',\n",
      "       'Cover 4 Zone', 'Cover 2 Zone', 'Cover 6 Zone'],\n",
      "      dtype='object'))\n",
      "tensor([0, 0, 0,  ..., 5, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "play_index_factor = pd.factorize(play_index['coverage'])\n",
    "print(play_index_factor)\n",
    "#play_index_factor['coverage'] = play_index_factor['coverage'].astype(object)\n",
    "play_index_factor = play_index_factor[0]\n",
    "play_index_factor = play_index_factor.tolist()\n",
    "coverage_tensor = torch.tensor(play_index_factor)\n",
    "#coverage_tensor = F.one_hot(coverage_tensor)\n",
    "#coverage_tensor = coverage_tensor.float()\n",
    "print(coverage_tensor)\n",
    "#print(F.one_hot(coverage_tensor,num_classes = 7))\n",
    "#coverage_tensor = F.one_hot(coverage_tensor,num_classes = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495d1fcd",
   "metadata": {},
   "source": [
    "Next, I seperate test and and training groups. I train on 785 plays and test on the other 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "487dcc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 12, 11, 5])\n",
      "tensor([5, 4, 4, 0, 5, 2, 5, 0, 2, 0, 0, 5, 6, 3, 2, 0, 2, 4, 2, 0, 4, 5, 2, 0,\n",
      "        0, 5, 4, 2, 2, 5, 5, 0, 2, 2, 4, 2, 5, 0, 4, 4, 2, 0, 2, 4, 5, 2, 0, 4,\n",
      "        0, 2, 6, 5, 2, 2, 2, 0, 0, 5, 0, 5, 6, 0, 4, 0, 2, 3, 2, 0, 0, 2, 2, 0,\n",
      "        2, 6, 4, 0, 4, 2, 4, 2, 3, 2, 2, 2, 0, 6, 2, 2, 4, 0, 0, 0, 0, 0, 0, 1,\n",
      "        2, 5, 2, 2, 0, 2, 6, 4, 0, 5, 0, 4, 6, 5, 2, 0, 2, 2, 0, 4, 5, 0, 4, 0,\n",
      "        0, 2, 6, 2, 2, 0, 0, 1, 4, 1, 2, 5, 0, 2, 6, 2, 0, 5, 0, 4, 4, 0, 0, 0,\n",
      "        2, 5, 0, 2, 5, 4, 3, 2, 0, 5, 0, 5, 2, 4, 2, 0, 2, 0, 2, 6, 2, 0, 2, 0,\n",
      "        4, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 0, 3, 0, 0, 0, 2, 5, 0, 5, 3, 2, 5, 4,\n",
      "        2, 0, 0, 3, 0, 2, 5, 4])\n",
      "torch.Size([785])\n"
     ]
    }
   ],
   "source": [
    "import random as rand\n",
    "test_size = 200\n",
    "rand.seed(80085)\n",
    "testId = rand.sample(range(985),k = test_size)\n",
    "#print(testId)\n",
    "test_data = data_tensor[testId]\n",
    "print(test_data.size())\n",
    "test_coverage = coverage_tensor[testId]\n",
    "test_coverage = test_coverage\n",
    "print(test_coverage)\n",
    "trainId = list(set(range(985))-set(testId)) + list(set(testId)-set(range(985)))\n",
    "#print(trainId)\n",
    "train_data = data_tensor[trainId]\n",
    "train_coverage = coverage_tensor[trainId]\n",
    "train_coverage = train_coverage\n",
    "\n",
    "#train_data=F.normalize(train_data)\n",
    "#test_data=F.normalize(test_data)\n",
    "print(train_coverage.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42bacd4",
   "metadata": {},
   "source": [
    "It's time for me to set up the dataset and dataloader from torch. These allow the data to be put in a way that allows the network to train and predict the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9151d5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.data_y = y_tensor\n",
    "        self.data_x = x_tensor\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.data_y.size()[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_x[idx,:], self.data_y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffb90099",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_data,train_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8cd8b5",
   "metadata": {},
   "source": [
    "I set the batch size at 64, which yeilds 13 batches, since 64 goes into 785 13 times.(not evenly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29c5eaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "train_dataLoad = DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
    "print(len(train_dataLoad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72896e9d",
   "metadata": {},
   "source": [
    "Here is where I define the network. I want the network to to end with 7 outputs, one for each possible coverage. I use leaky_relu as opposed to relu mainly because I had time to. When I learned about it I liked how it didn't create dead nuerons. And it didn't make my model take outrageously long to run so I was able to use it. I compared it to relu and leaky relu did yeild bettter results. I use cross entropy loss as my loss citerion. The cross entropy loss function applies softmax to my data, turning the output tensor into the probability that the given play is each of the coverages. The highest probability is assigned as the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b24abc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(pd.unique(play_index_factor).size)\n",
    "n_coverage = pd.unique(play_index_factor).size\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#convolutional!\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(12,128,1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(128,160,1)\n",
    "        self.fc1 = nn.Linear(320, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, n_coverage)\n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.leaky_relu(self.conv1(x)))\n",
    "        x = self.pool(F.leaky_relu(self.conv2(x)))\n",
    "        x = torch.flatten(x,1) \n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "       \n",
    "        #print(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dcb64c",
   "metadata": {},
   "source": [
    "I previously explained the reason for cross entropy as the loss criterion, and then for the optimizer I use a learning rate of .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4db3c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a647a",
   "metadata": {},
   "source": [
    "I chose 400 as the total amount of epochs, over many trial runs this seemed to be the best option, anything higher yielded little or no benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7537d0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 1.907\n",
      "[2,     1] loss: 1.690\n",
      "[3,     1] loss: 1.694\n",
      "[4,     1] loss: 1.553\n",
      "[5,     1] loss: 1.560\n",
      "[6,     1] loss: 1.444\n",
      "[7,     1] loss: 1.622\n",
      "[8,     1] loss: 1.561\n",
      "[9,     1] loss: 1.431\n",
      "[10,     1] loss: 1.581\n",
      "[11,     1] loss: 1.589\n",
      "[12,     1] loss: 1.482\n",
      "[13,     1] loss: 1.500\n",
      "[14,     1] loss: 1.447\n",
      "[15,     1] loss: 1.496\n",
      "[16,     1] loss: 1.554\n",
      "[17,     1] loss: 1.513\n",
      "[18,     1] loss: 1.386\n",
      "[19,     1] loss: 1.407\n",
      "[20,     1] loss: 1.436\n",
      "[21,     1] loss: 1.617\n",
      "[22,     1] loss: 1.372\n",
      "[23,     1] loss: 1.589\n",
      "[24,     1] loss: 1.689\n",
      "[25,     1] loss: 1.438\n",
      "[26,     1] loss: 1.516\n",
      "[27,     1] loss: 1.505\n",
      "[28,     1] loss: 1.645\n",
      "[29,     1] loss: 1.188\n",
      "[30,     1] loss: 1.541\n",
      "[31,     1] loss: 1.527\n",
      "[32,     1] loss: 1.557\n",
      "[33,     1] loss: 1.352\n",
      "[34,     1] loss: 1.420\n",
      "[35,     1] loss: 1.308\n",
      "[36,     1] loss: 1.398\n",
      "[37,     1] loss: 1.352\n",
      "[38,     1] loss: 1.436\n",
      "[39,     1] loss: 1.397\n",
      "[40,     1] loss: 1.277\n",
      "[41,     1] loss: 1.588\n",
      "[42,     1] loss: 1.376\n",
      "[43,     1] loss: 1.463\n",
      "[44,     1] loss: 1.470\n",
      "[45,     1] loss: 1.448\n",
      "[46,     1] loss: 1.390\n",
      "[47,     1] loss: 1.266\n",
      "[48,     1] loss: 1.547\n",
      "[49,     1] loss: 1.324\n",
      "[50,     1] loss: 1.292\n",
      "[51,     1] loss: 1.513\n",
      "[52,     1] loss: 1.289\n",
      "[53,     1] loss: 1.332\n",
      "[54,     1] loss: 1.484\n",
      "[55,     1] loss: 1.341\n",
      "[56,     1] loss: 1.351\n",
      "[57,     1] loss: 1.336\n",
      "[58,     1] loss: 1.169\n",
      "[59,     1] loss: 1.535\n",
      "[60,     1] loss: 1.248\n",
      "[61,     1] loss: 1.268\n",
      "[62,     1] loss: 1.244\n",
      "[63,     1] loss: 1.234\n",
      "[64,     1] loss: 1.252\n",
      "[65,     1] loss: 1.204\n",
      "[66,     1] loss: 1.324\n",
      "[67,     1] loss: 1.165\n",
      "[68,     1] loss: 1.340\n",
      "[69,     1] loss: 1.249\n",
      "[70,     1] loss: 1.220\n",
      "[71,     1] loss: 1.395\n",
      "[72,     1] loss: 1.297\n",
      "[73,     1] loss: 1.245\n",
      "[74,     1] loss: 1.209\n",
      "[75,     1] loss: 1.268\n",
      "[76,     1] loss: 1.217\n",
      "[77,     1] loss: 1.294\n",
      "[78,     1] loss: 1.328\n",
      "[79,     1] loss: 1.392\n",
      "[80,     1] loss: 1.478\n",
      "[81,     1] loss: 1.357\n",
      "[82,     1] loss: 1.357\n",
      "[83,     1] loss: 1.263\n",
      "[84,     1] loss: 1.295\n",
      "[85,     1] loss: 1.090\n",
      "[86,     1] loss: 1.214\n",
      "[87,     1] loss: 1.215\n",
      "[88,     1] loss: 1.072\n",
      "[89,     1] loss: 1.401\n",
      "[90,     1] loss: 1.203\n",
      "[91,     1] loss: 1.398\n",
      "[92,     1] loss: 1.208\n",
      "[93,     1] loss: 1.196\n",
      "[94,     1] loss: 1.189\n",
      "[95,     1] loss: 1.093\n",
      "[96,     1] loss: 1.214\n",
      "[97,     1] loss: 1.221\n",
      "[98,     1] loss: 1.306\n",
      "[99,     1] loss: 1.246\n",
      "[100,     1] loss: 1.179\n",
      "[101,     1] loss: 1.132\n",
      "[102,     1] loss: 1.173\n",
      "[103,     1] loss: 1.097\n",
      "[104,     1] loss: 1.107\n",
      "[105,     1] loss: 1.218\n",
      "[106,     1] loss: 1.085\n",
      "[107,     1] loss: 0.999\n",
      "[108,     1] loss: 0.920\n",
      "[109,     1] loss: 1.377\n",
      "[110,     1] loss: 1.056\n",
      "[111,     1] loss: 1.041\n",
      "[112,     1] loss: 0.944\n",
      "[113,     1] loss: 1.140\n",
      "[114,     1] loss: 1.183\n",
      "[115,     1] loss: 0.878\n",
      "[116,     1] loss: 1.047\n",
      "[117,     1] loss: 1.203\n",
      "[118,     1] loss: 1.099\n",
      "[119,     1] loss: 1.198\n",
      "[120,     1] loss: 1.028\n",
      "[121,     1] loss: 1.002\n",
      "[122,     1] loss: 0.974\n",
      "[123,     1] loss: 1.133\n",
      "[124,     1] loss: 1.238\n",
      "[125,     1] loss: 1.143\n",
      "[126,     1] loss: 0.945\n",
      "[127,     1] loss: 0.890\n",
      "[128,     1] loss: 0.817\n",
      "[129,     1] loss: 0.968\n",
      "[130,     1] loss: 0.995\n",
      "[131,     1] loss: 1.028\n",
      "[132,     1] loss: 1.070\n",
      "[133,     1] loss: 0.993\n",
      "[134,     1] loss: 0.908\n",
      "[135,     1] loss: 0.898\n",
      "[136,     1] loss: 1.029\n",
      "[137,     1] loss: 1.125\n",
      "[138,     1] loss: 0.913\n",
      "[139,     1] loss: 1.050\n",
      "[140,     1] loss: 0.960\n",
      "[141,     1] loss: 0.973\n",
      "[142,     1] loss: 0.901\n",
      "[143,     1] loss: 0.971\n",
      "[144,     1] loss: 0.890\n",
      "[145,     1] loss: 0.941\n",
      "[146,     1] loss: 1.022\n",
      "[147,     1] loss: 1.163\n",
      "[148,     1] loss: 1.121\n",
      "[149,     1] loss: 0.994\n",
      "[150,     1] loss: 1.041\n",
      "[151,     1] loss: 0.988\n",
      "[152,     1] loss: 0.915\n",
      "[153,     1] loss: 0.900\n",
      "[154,     1] loss: 0.949\n",
      "[155,     1] loss: 1.096\n",
      "[156,     1] loss: 0.850\n",
      "[157,     1] loss: 1.043\n",
      "[158,     1] loss: 0.817\n",
      "[159,     1] loss: 0.831\n",
      "[160,     1] loss: 1.030\n",
      "[161,     1] loss: 0.894\n",
      "[162,     1] loss: 0.980\n",
      "[163,     1] loss: 0.964\n",
      "[164,     1] loss: 1.082\n",
      "[165,     1] loss: 1.008\n",
      "[166,     1] loss: 0.826\n",
      "[167,     1] loss: 0.808\n",
      "[168,     1] loss: 0.892\n",
      "[169,     1] loss: 0.728\n",
      "[170,     1] loss: 0.779\n",
      "[171,     1] loss: 0.738\n",
      "[172,     1] loss: 0.747\n",
      "[173,     1] loss: 0.734\n",
      "[174,     1] loss: 0.965\n",
      "[175,     1] loss: 0.662\n",
      "[176,     1] loss: 0.749\n",
      "[177,     1] loss: 1.015\n",
      "[178,     1] loss: 0.786\n",
      "[179,     1] loss: 0.751\n",
      "[180,     1] loss: 0.681\n",
      "[181,     1] loss: 1.096\n",
      "[182,     1] loss: 0.755\n",
      "[183,     1] loss: 0.992\n",
      "[184,     1] loss: 0.889\n",
      "[185,     1] loss: 0.657\n",
      "[186,     1] loss: 0.657\n",
      "[187,     1] loss: 0.768\n",
      "[188,     1] loss: 0.786\n",
      "[189,     1] loss: 0.814\n",
      "[190,     1] loss: 0.909\n",
      "[191,     1] loss: 0.612\n",
      "[192,     1] loss: 0.632\n",
      "[193,     1] loss: 0.534\n",
      "[194,     1] loss: 0.595\n",
      "[195,     1] loss: 0.775\n",
      "[196,     1] loss: 1.044\n",
      "[197,     1] loss: 0.882\n",
      "[198,     1] loss: 0.773\n",
      "[199,     1] loss: 0.710\n",
      "[200,     1] loss: 0.719\n",
      "[201,     1] loss: 0.623\n",
      "[202,     1] loss: 0.579\n",
      "[203,     1] loss: 0.503\n",
      "[204,     1] loss: 0.451\n",
      "[205,     1] loss: 0.737\n",
      "[206,     1] loss: 0.722\n",
      "[207,     1] loss: 0.720\n",
      "[208,     1] loss: 0.684\n",
      "[209,     1] loss: 0.676\n",
      "[210,     1] loss: 0.649\n",
      "[211,     1] loss: 0.582\n",
      "[212,     1] loss: 0.576\n",
      "[213,     1] loss: 0.588\n",
      "[214,     1] loss: 0.598\n",
      "[215,     1] loss: 0.439\n",
      "[216,     1] loss: 0.464\n",
      "[217,     1] loss: 0.529\n",
      "[218,     1] loss: 0.631\n",
      "[219,     1] loss: 0.534\n",
      "[220,     1] loss: 0.581\n",
      "[221,     1] loss: 0.565\n",
      "[222,     1] loss: 0.644\n",
      "[223,     1] loss: 0.619\n",
      "[224,     1] loss: 0.478\n",
      "[225,     1] loss: 0.439\n",
      "[226,     1] loss: 0.621\n",
      "[227,     1] loss: 0.990\n",
      "[228,     1] loss: 0.689\n",
      "[229,     1] loss: 0.640\n",
      "[230,     1] loss: 0.437\n",
      "[231,     1] loss: 0.517\n",
      "[232,     1] loss: 0.530\n",
      "[233,     1] loss: 0.377\n",
      "[234,     1] loss: 0.386\n",
      "[235,     1] loss: 0.405\n",
      "[236,     1] loss: 0.453\n",
      "[237,     1] loss: 0.431\n",
      "[238,     1] loss: 0.426\n",
      "[239,     1] loss: 0.396\n",
      "[240,     1] loss: 0.436\n",
      "[241,     1] loss: 0.323\n",
      "[242,     1] loss: 0.432\n",
      "[243,     1] loss: 0.363\n",
      "[244,     1] loss: 0.257\n",
      "[245,     1] loss: 0.384\n",
      "[246,     1] loss: 0.455\n",
      "[247,     1] loss: 0.350\n",
      "[248,     1] loss: 0.438\n",
      "[249,     1] loss: 0.561\n",
      "[250,     1] loss: 0.378\n",
      "[251,     1] loss: 0.342\n",
      "[252,     1] loss: 0.291\n",
      "[253,     1] loss: 0.453\n",
      "[254,     1] loss: 0.284\n",
      "[255,     1] loss: 0.353\n",
      "[256,     1] loss: 0.820\n",
      "[257,     1] loss: 0.374\n",
      "[258,     1] loss: 0.223\n",
      "[259,     1] loss: 0.318\n",
      "[260,     1] loss: 0.285\n",
      "[261,     1] loss: 0.344\n",
      "[262,     1] loss: 0.299\n",
      "[263,     1] loss: 0.220\n",
      "[264,     1] loss: 0.362\n",
      "[265,     1] loss: 0.540\n",
      "[266,     1] loss: 0.463\n",
      "[267,     1] loss: 0.426\n",
      "[268,     1] loss: 0.350\n",
      "[269,     1] loss: 0.349\n",
      "[270,     1] loss: 0.330\n",
      "[271,     1] loss: 0.235\n",
      "[272,     1] loss: 0.303\n",
      "[273,     1] loss: 0.267\n",
      "[274,     1] loss: 0.237\n",
      "[275,     1] loss: 0.232\n",
      "[276,     1] loss: 0.273\n",
      "[277,     1] loss: 0.215\n",
      "[278,     1] loss: 0.211\n",
      "[279,     1] loss: 0.185\n",
      "[280,     1] loss: 0.189\n",
      "[281,     1] loss: 0.168\n",
      "[282,     1] loss: 0.228\n",
      "[283,     1] loss: 0.155\n",
      "[284,     1] loss: 0.243\n",
      "[285,     1] loss: 0.173\n",
      "[286,     1] loss: 0.289\n",
      "[287,     1] loss: 0.174\n",
      "[288,     1] loss: 0.154\n",
      "[289,     1] loss: 0.250\n",
      "[290,     1] loss: 0.313\n",
      "[291,     1] loss: 0.183\n",
      "[292,     1] loss: 0.157\n",
      "[293,     1] loss: 0.158\n",
      "[294,     1] loss: 0.216\n",
      "[295,     1] loss: 0.149\n",
      "[296,     1] loss: 0.128\n",
      "[297,     1] loss: 0.205\n",
      "[298,     1] loss: 0.178\n",
      "[299,     1] loss: 0.108\n",
      "[300,     1] loss: 0.100\n",
      "[301,     1] loss: 0.073\n",
      "[302,     1] loss: 0.107\n",
      "[303,     1] loss: 0.090\n",
      "[304,     1] loss: 0.162\n",
      "[305,     1] loss: 0.144\n",
      "[306,     1] loss: 0.119\n",
      "[307,     1] loss: 0.139\n",
      "[308,     1] loss: 0.079\n",
      "[309,     1] loss: 0.070\n",
      "[310,     1] loss: 0.093\n",
      "[311,     1] loss: 0.093\n",
      "[312,     1] loss: 0.179\n",
      "[313,     1] loss: 0.113\n",
      "[314,     1] loss: 0.159\n",
      "[315,     1] loss: 0.108\n",
      "[316,     1] loss: 0.097\n",
      "[317,     1] loss: 0.133\n",
      "[318,     1] loss: 0.117\n",
      "[319,     1] loss: 0.245\n",
      "[320,     1] loss: 0.194\n",
      "[321,     1] loss: 0.105\n",
      "[322,     1] loss: 0.149\n",
      "[323,     1] loss: 0.099\n",
      "[324,     1] loss: 0.050\n",
      "[325,     1] loss: 0.065\n",
      "[326,     1] loss: 0.067\n",
      "[327,     1] loss: 0.113\n",
      "[328,     1] loss: 0.057\n",
      "[329,     1] loss: 0.073\n",
      "[330,     1] loss: 0.091\n",
      "[331,     1] loss: 0.051\n",
      "[332,     1] loss: 0.049\n",
      "[333,     1] loss: 0.059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[334,     1] loss: 0.044\n",
      "[335,     1] loss: 0.063\n",
      "[336,     1] loss: 0.053\n",
      "[337,     1] loss: 0.049\n",
      "[338,     1] loss: 0.046\n",
      "[339,     1] loss: 0.059\n",
      "[340,     1] loss: 0.044\n",
      "[341,     1] loss: 0.036\n",
      "[342,     1] loss: 0.034\n",
      "[343,     1] loss: 0.050\n",
      "[344,     1] loss: 0.049\n",
      "[345,     1] loss: 0.037\n",
      "[346,     1] loss: 0.046\n",
      "[347,     1] loss: 0.052\n",
      "[348,     1] loss: 0.044\n",
      "[349,     1] loss: 0.045\n",
      "[350,     1] loss: 0.050\n",
      "[351,     1] loss: 0.033\n",
      "[352,     1] loss: 0.038\n",
      "[353,     1] loss: 0.041\n",
      "[354,     1] loss: 0.035\n",
      "[355,     1] loss: 0.037\n",
      "[356,     1] loss: 0.047\n",
      "[357,     1] loss: 0.035\n",
      "[358,     1] loss: 0.037\n",
      "[359,     1] loss: 0.059\n",
      "[360,     1] loss: 0.074\n",
      "[361,     1] loss: 0.037\n",
      "[362,     1] loss: 0.030\n",
      "[363,     1] loss: 0.043\n",
      "[364,     1] loss: 0.024\n",
      "[365,     1] loss: 0.024\n",
      "[366,     1] loss: 0.046\n",
      "[367,     1] loss: 0.031\n",
      "[368,     1] loss: 0.024\n",
      "[369,     1] loss: 0.033\n",
      "[370,     1] loss: 0.022\n",
      "[371,     1] loss: 0.028\n",
      "[372,     1] loss: 0.030\n",
      "[373,     1] loss: 0.020\n",
      "[374,     1] loss: 0.024\n",
      "[375,     1] loss: 0.024\n",
      "[376,     1] loss: 0.019\n",
      "[377,     1] loss: 0.023\n",
      "[378,     1] loss: 0.023\n",
      "[379,     1] loss: 0.025\n",
      "[380,     1] loss: 0.037\n",
      "[381,     1] loss: 0.013\n",
      "[382,     1] loss: 0.016\n",
      "[383,     1] loss: 0.021\n",
      "[384,     1] loss: 0.024\n",
      "[385,     1] loss: 0.024\n",
      "[386,     1] loss: 0.020\n",
      "[387,     1] loss: 0.025\n",
      "[388,     1] loss: 0.023\n",
      "[389,     1] loss: 0.020\n",
      "[390,     1] loss: 0.023\n",
      "[391,     1] loss: 0.019\n",
      "[392,     1] loss: 0.017\n",
      "[393,     1] loss: 0.018\n",
      "[394,     1] loss: 0.023\n",
      "[395,     1] loss: 0.036\n",
      "[396,     1] loss: 0.021\n",
      "[397,     1] loss: 0.012\n",
      "[398,     1] loss: 0.013\n",
      "[399,     1] loss: 0.025\n",
      "[400,     1] loss: 0.020\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "for epoch in range(400):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataLoad):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 36== 0:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss ))\n",
    "            running_loss = 0.0\n",
    "#print(inputs, labels)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "120b9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "def5b4f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64]) torch.Size([64])\n",
      "tensor([0, 0, 2, 0, 0, 2, 6, 2, 2, 2, 2, 0, 5, 4, 5, 0, 0, 5, 2, 2, 0, 0, 2, 2,\n",
      "        0, 4, 4, 0, 0, 5, 4, 5, 5, 4, 0, 2, 0, 3, 4, 6, 0, 2, 0, 5, 0, 2, 0, 0,\n",
      "        0, 2, 2, 5, 6, 0, 2, 0, 4, 3, 0, 5, 0, 5, 2, 4])\n",
      "tensor([0, 0, 2, 0, 4, 0, 4, 2, 2, 2, 0, 0, 2, 0, 2, 0, 0, 6, 2, 2, 0, 3, 0, 2,\n",
      "        0, 5, 0, 4, 0, 5, 0, 0, 5, 4, 0, 4, 0, 5, 4, 0, 0, 5, 2, 5, 2, 0, 0, 0,\n",
      "        0, 2, 2, 0, 4, 0, 2, 2, 4, 2, 0, 4, 4, 2, 2, 4])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "tensor([2, 2, 0, 4, 2, 0, 4, 0, 2, 3, 2, 2, 5, 6, 4, 3, 3, 4, 4, 4, 2, 3, 2, 5,\n",
      "        1, 5, 2, 6, 0, 0, 0, 0, 2, 0, 4, 5, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2,\n",
      "        0, 2, 0, 4, 0, 2, 2, 2, 2, 4, 3, 4, 0, 2, 2, 2])\n",
      "tensor([2, 2, 0, 4, 2, 2, 0, 4, 6, 2, 2, 2, 5, 5, 6, 3, 2, 0, 0, 5, 2, 2, 2, 5,\n",
      "        2, 5, 2, 5, 0, 5, 0, 0, 2, 0, 4, 2, 0, 0, 0, 2, 3, 2, 0, 0, 4, 5, 0, 2,\n",
      "        4, 2, 0, 4, 0, 2, 5, 2, 2, 4, 4, 0, 2, 2, 2, 2])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "tensor([2, 0, 2, 2, 2, 2, 4, 0, 0, 5, 4, 2, 4, 2, 6, 5, 4, 2, 6, 4, 0, 5, 0, 0,\n",
      "        0, 0, 0, 2, 6, 5, 1, 0, 6, 2, 2, 0, 4, 0, 0, 0, 2, 2, 2, 2, 5, 5, 5, 0,\n",
      "        5, 0, 6, 2, 2, 0, 5, 0, 4, 0, 5, 2, 2, 5, 4, 0])\n",
      "tensor([0, 0, 2, 0, 2, 2, 4, 0, 6, 0, 4, 0, 6, 2, 0, 4, 6, 2, 4, 4, 2, 4, 6, 4,\n",
      "        2, 2, 2, 2, 6, 0, 1, 0, 0, 2, 0, 0, 4, 2, 5, 0, 1, 2, 0, 3, 5, 2, 5, 0,\n",
      "        5, 0, 4, 0, 2, 6, 5, 0, 4, 0, 0, 0, 2, 0, 0, 4])\n",
      "torch.Size([8]) torch.Size([8])\n",
      "tensor([5, 0, 0, 2, 4, 5, 0, 1])\n",
      "tensor([4, 0, 0, 2, 0, 6, 0, 2])\n",
      "Accuracy of the network on the 985 test plays: 53 %\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "test_dataset = Dataset(test_data,test_coverage)\n",
    "test_dataLoad = DataLoader(test_dataset, batch_size = 64, shuffle = True)\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_dataLoad:\n",
    "        plays, labels = data\n",
    "        \n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(plays)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        print(predicted.shape,labels.shape)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        print(labels)\n",
    "        print(predicted)\n",
    "        \n",
    "print('Accuracy of the network on the 985 test plays: %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e22ef7",
   "metadata": {},
   "source": [
    "For my first neural network I am very happy with what I made. While 53% doesn't scream success it is greater than random, or 1/7. Ben's model had 78% accuracy, and as I learn more about neural networks and how to build them, along with using more frames over the course of the play and including orientation to the qb, I believe I could improve my model greatly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
